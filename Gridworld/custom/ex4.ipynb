{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$ \\theta = \\{\\psi, \\sigma_1, \\sigma_2\\} $"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$p_{\\theta}(s_{t+1}|s_t, a_t) = \\{_{pdf(x_{t+1}, \\; (x_t+a_t^0), \\; \\theta_2) \\; \\times \\; pdf(y_{t+1}, \\; (y_t+a_t^1), \\; \\theta_2) \\; if \\; x_t >= \\theta_0}^{pdf(x_{t+1}, \\; (x_t+a_t^0), \\; \\theta_1) \\; \\times \\; pdf(y_{t+1}, \\; (y_t+a_t^1), \\; \\theta_1) \\; if \\; x_t < \\theta_0}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[((1, 1), (1, 0)),\n",
       " (array([3.95218613, 2.51070932]), (0, -1)),\n",
       " (array([-0.71340399, -2.95685681]), (0, 1)),\n",
       " (array([-4.417886  , -3.29214511]), (1, 0)),\n",
       " (array([-5.61129041, -1.86457798]), (1, 0)),\n",
       " (array([-2.35759381, -2.6355726 ]), (0, -1)),\n",
       " (array([-1.6054336 , -2.56177237]), (0, 1)),\n",
       " (array([-0.26629409,  1.24629043]), (0, -1)),\n",
       " (array([-2.38207098,  1.12319154]), (-1, 0)),\n",
       " (array([-4.44492119,  0.95513588]), (0, 1)),\n",
       " (array([-5.96026554,  1.6555269 ]), (-1, 0)),\n",
       " (array([-9.90436861,  1.69353749]), (0, -1)),\n",
       " (array([-11.67902551,   1.2540445 ]), (1, 0)),\n",
       " (array([-7.89248626,  1.20249277]), (0, -1)),\n",
       " (array([-6.50464646,  2.29716939]), (1, 0)),\n",
       " (array([-2.77681646,  4.54712814]), (1, 0)),\n",
       " (array([-2.45561269,  6.8108771 ]), (1, 0)),\n",
       " (array([-4.54204735,  3.91584188]), (0, 1)),\n",
       " (array([-3.49036892,  4.36620011]), (0, -1)),\n",
       " (array([-6.12663044,  3.27603272]), (1, 0)),\n",
       " (array([-7.50190176,  4.18168552]), (0, -1)),\n",
       " (array([-7.79802635,  2.53194281]), (-1, 0)),\n",
       " (array([-10.72056569,   0.59631109]), (0, -1)),\n",
       " (array([-8.19402958,  0.12991363]), (0, 1)),\n",
       " (array([-7.48032859,  0.91544779]), (-1, 0)),\n",
       " (array([-10.58186247,   1.5766863 ]), (0, 1)),\n",
       " (array([-12.28787605,   1.95211838]), (0, 1)),\n",
       " (array([-12.39408689,   4.77266236]), (0, -1)),\n",
       " (array([-10.6407437 ,   3.84688656]), (0, -1)),\n",
       " (array([-13.25395215,   4.42056569]), (-1, 0)),\n",
       " (array([-12.6173306 ,   3.29520121]), (0, -1)),\n",
       " (array([-15.10091947,  -0.65039779]), (1, 0)),\n",
       " (array([-12.30481801,   0.82888363]), (0, -1)),\n",
       " (array([-11.43110508,   4.27613546]), (1, 0)),\n",
       " (array([-12.8300028 ,   4.68856598]), (0, -1)),\n",
       " (array([-13.97132882,   5.94526356]), (0, -1)),\n",
       " (array([-13.26343875,   6.97257231]), (-1, 0)),\n",
       " (array([-13.63932118,   7.95095592]), (0, 1)),\n",
       " (array([-14.33724492,   8.83365541]), (0, -1)),\n",
       " (array([-12.34367881,   7.5569793 ]), (1, 0)),\n",
       " (array([-11.43469246,   6.91486598]), (0, 1)),\n",
       " (array([-11.4557524 ,   5.86426706]), (-1, 0)),\n",
       " (array([-10.63252551,   7.43679049]), (0, 1)),\n",
       " (array([-10.6850469 ,   6.13803363]), (0, 1)),\n",
       " (array([-13.49106913,   7.4792757 ]), (-1, 0)),\n",
       " (array([-14.95086388,   7.06393947]), (0, -1)),\n",
       " (array([-17.78715459,   5.51943705]), (0, -1)),\n",
       " (array([-17.04763065,   1.48444956]), (0, -1)),\n",
       " (array([-18.83434221,  -0.70896808]), (-1, 0)),\n",
       " (array([-18.82404218,   1.95086088]), (0, 1)),\n",
       " (array([-16.41935983,   2.71878829]), (1, 0)),\n",
       " (array([-18.86581204,   1.16035586]), (-1, 0)),\n",
       " (array([-22.59779584,  -2.88419909]), (-1, 0)),\n",
       " (array([-24.1801601 ,  -0.28415409]), (0, -1)),\n",
       " (array([-23.55139711,  -1.22998211]), (0, -1)),\n",
       " (array([-22.17518234,   0.62456118]), (-1, 0)),\n",
       " (array([-24.4888211 ,   0.31600764]), (-1, 0)),\n",
       " (array([-24.25680465,  -0.22385279]), (0, 1)),\n",
       " (array([-29.0913907 ,  -4.71205134]), (0, 1)),\n",
       " (array([-28.69140069,  -0.39396224]), (-1, 0)),\n",
       " (array([-29.51952348,  -2.63344963]), (0, -1)),\n",
       " (array([-28.20007628,  -6.19415516]), (0, -1)),\n",
       " (array([-27.36425321, -10.59842045]), (0, -1)),\n",
       " (array([-23.21359352, -11.62431716]), (-1, 0)),\n",
       " (array([-25.46248186, -11.23901886]), (0, -1)),\n",
       " (array([-27.85996089,  -9.9038435 ]), (0, -1)),\n",
       " (array([-28.0027069 , -10.47966115]), (0, -1)),\n",
       " (array([-27.20227755, -11.56788128]), (-1, 0)),\n",
       " (array([-28.72562376,  -9.06471094]), (1, 0)),\n",
       " (array([-28.80821763,  -7.65479424]), (-1, 0)),\n",
       " (array([-26.00928634,  -4.82634939]), (-1, 0)),\n",
       " (array([-27.33589636,  -7.27582099]), (0, -1)),\n",
       " (array([-28.16976183,  -7.45669945]), (-1, 0)),\n",
       " (array([-29.36572512,  -7.74908764]), (0, 1)),\n",
       " (array([-27.56313942,  -4.98184989]), (0, -1)),\n",
       " (array([-23.20035874,  -4.47417303]), (0, -1)),\n",
       " (array([-22.56600949,  -7.180318  ]), (0, -1)),\n",
       " (array([-25.96505921,  -6.38028222]), (-1, 0)),\n",
       " (array([-25.92523292,  -5.3703068 ]), (0, 1)),\n",
       " (array([-28.23628852,  -6.70977028]), (1, 0)),\n",
       " (array([-29.27949385,  -5.98752856]), (1, 0)),\n",
       " (array([-30.43725817,  -3.87770493]), (1, 0)),\n",
       " (array([-34.3849929,  -5.2799503]), (-1, 0)),\n",
       " (array([-35.48544546,  -7.14057122]), (-1, 0)),\n",
       " (array([-42.13083647,  -4.48039185]), (0, 1)),\n",
       " (array([-44.56393795,  -6.70924431]), (0, 1)),\n",
       " (array([-46.69446367,  -7.28838676]), (-1, 0)),\n",
       " (array([-47.66004361,  -3.96117974]), (0, 1)),\n",
       " (array([-47.12686635,  -3.99132652]), (0, -1)),\n",
       " (array([-49.35864581,  -7.0217211 ]), (0, -1)),\n",
       " (array([-48.58119728,  -9.06262198]), (1, 0)),\n",
       " (array([-47.89124346, -12.30808078]), (0, -1)),\n",
       " (array([-52.83781039, -14.89937333]), (0, -1)),\n",
       " (array([-55.73665618, -14.68343601]), (1, 0)),\n",
       " (array([-54.16568157, -14.1553322 ]), (0, -1)),\n",
       " (array([-55.06416667, -18.23159638]), (1, 0)),\n",
       " (array([-58.20362059, -16.51822983]), (-1, 0)),\n",
       " (array([-60.13011832, -16.12422852]), (1, 0)),\n",
       " (array([-61.84142238, -16.80852518]), (0, -1)),\n",
       " (array([-64.6379298 , -20.05204644]), (0, 1))]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "\n",
    "theta = [5, 2, 4]\n",
    "\n",
    "def transiction(s, a, p=2):\n",
    "    return np.array(s) + np.array(a) + np.random.normal(0, p, 2)\n",
    "\n",
    "def pickModel(s, theta):\n",
    "    psi, *m = theta\n",
    "    x,_ = s\n",
    "    return m[int(x >= psi)]\n",
    "\n",
    "def generate(s, n, theta):   \n",
    "    for _ in range(n):\n",
    "        a = random.choice([(1,0), (-1,0), (0,1), (0,-1)])\n",
    "        yield s, a\n",
    "        s = transiction(s, a, pickModel(s, theta))\n",
    "\n",
    "s_0 = (1,1)\n",
    "O = [(s, a) for (s, a) in generate(s_0, 100, theta)]\n",
    "O"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$p_{\\theta}(s_{t+1}|s_t, a_t) = pdf(s^x_{t+1}, (s^x_t + a_t^x), \\theta_2) \\; \\times \\; pdf(s^y_{t+1}, (s^y_t+a_t^y), \\theta_2) \\; \\times \\; \\mathbb{I}_{s^x_t >= \\theta_0} + pdf(s^x_{t+1}, (s^x_t+a_t^x), \\theta_1) \\; \\times \\; pdf(s^y_{t+1}, (s^y_t+a_t^y), \\theta_1) \\; \\times \\; \\mathbb{I}_{s^x_t < \\theta_0}$\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "$p_{\\theta}(s_{t+1}|s_t, a_t) = pdf(s^x_{t+1}, (s^x_t+a_t^x), \\theta_2) \\; \\times \\; pdf(s^y_{t+1}, (s^y_t+a_t^y), \\theta_2) \\; \\times \\; \\frac{1}{1+e^{\\beta_{s_t}}}$ <br>\n",
    "$ \\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\; $\n",
    "$ + \\; pdf(s^x_{t+1}, (s^x_t+a_t^x), \\theta_1) \\; \\times \\; pdf(s^y_{t+1}, (s^y_t+a_t^y), \\theta_1) \\; \\times \\; (1 - \\frac{1}{1+e^{\\beta_{s_t}}})$ \n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$ \\beta_{s_t} = f(s^x_t, s^y_t; \\theta)$"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\cal{L}[\\xi | \\theta] = \\sum_{t=0}^{\\mathrm{T}-1} log \\; p_{\\theta}(s_{t+1}|s_t, a_t) - \\gamma$"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$ \\gamma = \\sum_t \\sum_{s\\;vizinho\\;s_t} (1-p(s)) * p(s_t) + p(s) * (1-p(s_t)) $"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$ p(s) =  $"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$ \\gamma = \\sum_t \\sum_{s\\;vizinho\\;s_t} KL(p(s), p(s_t))$ <br>\n",
    "KL = Kullback-Leibler "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$ \\gamma = \\sum_t \\sum_{s} KL(p(s), p(s_t)) * e^{-|s_t - s|} $"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "446124.7741454962"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from scipy.stats import norm\n",
    "\n",
    "def ll(O, theta, c=10):\n",
    "    psi, sig1, sig2 = theta\n",
    "\n",
    "    beta = lambda x,y, theta: c * (theta-x)\n",
    "    p = lambda x_,y_, x,y, ax,ay: ( \n",
    "        norm.pdf(x_, (x + ax), sig2)  \n",
    "        * norm.pdf(y_, (y + ay), sig2) \n",
    "        * (1/(1 + np.e **beta(x,y, psi)))\n",
    "        +\n",
    "        norm.pdf(x_, (x + ax), sig1) \n",
    "        * norm.pdf(y_, (y + ay), sig1)\n",
    "        * (1 - (1/(1 + np.e **beta(x,y, psi))))\n",
    "    )\n",
    "\n",
    "    ps = lambda x, y: x+y #?\n",
    "    d = lambda p1, p2: (1-p1)*p2 + p1*(1-p2)\n",
    "\n",
    "    S = set([(x,y) for ((x,y), _) in O])\n",
    "    gamma = np.sum([d(ps(x,y), ps(xt,yt)) for t,((xt,yt),_) in enumerate(O) for (x,y) in S if (abs(x-xt)<1.5 and abs(y-yt)<1.5)])\n",
    "    return np.sum([np.log(p(*st, *O[t-1][0], *O[t-1][1])) for t,(st,_) in enumerate(O) if 0 < t]) - gamma\n",
    "\n",
    "ll(O, (5, 2, 4), c=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\guilherme.albarrans\\AppData\\Local\\Temp\\ipykernel_57128\\2691689637.py:10: RuntimeWarning: overflow encountered in scalar power\n",
      "  * (1/(1 + np.e **beta(x,y, psi)))\n",
      "C:\\Users\\guilherme.albarrans\\AppData\\Local\\Temp\\ipykernel_57128\\2691689637.py:14: RuntimeWarning: overflow encountered in scalar power\n",
      "  * (1 - (1/(1 + np.e **beta(x,y, psi))))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2, 2, 5) 446127.3225310683\n"
     ]
    }
   ],
   "source": [
    "# best_theta = 0\n",
    "# best_ll = -np.inf\n",
    "\n",
    "# for psi in range(10):\n",
    "#     for p1 in range(10):\n",
    "#         for p2 in range(10):\n",
    "#             theta = (psi+1, p1+1, p2+1)\n",
    "#             new_ll = ll(O, theta)\n",
    "#             if new_ll > best_ll:\n",
    "#                 best_ll = new_ll\n",
    "#                 best_theta = theta\n",
    "\n",
    "# print(best_theta, best_ll)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "like:  tensor(-440.3043, dtype=torch.float64, grad_fn=<SumBackward0>)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch.autograd import Variable\n",
    "from torch.distributions import normal\n",
    "\n",
    "s = Variable(torch.from_numpy(np.array(O)[:-1,0])).type(torch.DoubleTensor)\n",
    "a = Variable(torch.from_numpy(np.array(O)[:-1,1])).type(torch.IntTensor)\n",
    "s_ = Variable(torch.from_numpy(np.array(O)[1:,0])).type(torch.DoubleTensor)\n",
    "\n",
    "p = Variable(torch.Tensor([5, 2, 4]), requires_grad=True)\n",
    "beta = 10\n",
    "\n",
    "# loglike = torch.sum(normal.Normal(s[:,0] + a[:,0], torch.where(s[:,0] >= p[0], p[2], p[1])).log_prob(s_[:,0]) + normal.Normal(s[:,1] + a[:,1], torch.where(s[:,0] >= p[0], p[2], p[1])).log_prob(s_[:,1]))\n",
    "loglike = torch.sum(\n",
    "        torch.log(\n",
    "        torch.exp(normal.Normal(s[:,0] + a[:,0], p[2]).log_prob(s_[:,0]))\n",
    "        * torch.exp(normal.Normal(s[:,1] + a[:,1], p[2]).log_prob(s_[:,1]))\n",
    "        * torch.sigmoid(-beta * (p[0]-s[:,0]))\n",
    "        + \n",
    "        torch.exp(normal.Normal(s[:,0] + a[:,0], p[1]).log_prob(s_[:,0]))\n",
    "        * torch.exp(normal.Normal(s[:,1] + a[:,1], p[1]).log_prob(s_[:,1]))\n",
    "        * (1 - torch.sigmoid(-beta * (p[0]-s[:,0])))\n",
    "    )\n",
    ")\n",
    "\n",
    "loglike.backward()\n",
    "gradiente = p.grad.clone()\n",
    "print('like: ', loglike)\n",
    "# gradiente.numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loglike  = -439.8474988484604 p = [5.2120705 1.9280896 4.0618114]\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch.autograd import Variable\n",
    "from torch.distributions import normal\n",
    "\n",
    "s = Variable(torch.from_numpy(np.array(O)[:-1,0])).type(torch.DoubleTensor)\n",
    "a = Variable(torch.from_numpy(np.array(O)[:-1,1])).type(torch.IntTensor)\n",
    "s_ = Variable(torch.from_numpy(np.array(O)[1:,0])).type(torch.DoubleTensor)\n",
    "\n",
    "p = Variable(torch.randn(3), requires_grad=True)\n",
    "beta = 10\n",
    "\n",
    "while p[1] <= 0 or p[2] <= 0:\n",
    "    p = Variable(torch.randn(3), requires_grad=True)\n",
    "\n",
    "e_margin = 1e-6\n",
    "learning_rate = 0.002\n",
    "e = - 1e+10\n",
    "for t in range(10000):\n",
    "    # print(p)\n",
    "    \n",
    "    loglike = torch.sum(\n",
    "        torch.log(\n",
    "            torch.exp(normal.Normal(s[:,0] + a[:,0], p[2]).log_prob(s_[:,0]))\n",
    "            * torch.exp(normal.Normal(s[:,1] + a[:,1], p[2]).log_prob(s_[:,1]))\n",
    "            * torch.sigmoid(-beta * (p[0]-s[:,0]))\n",
    "            + \n",
    "            torch.exp(normal.Normal(s[:,0] + a[:,0], p[1]).log_prob(s_[:,0]))\n",
    "            * torch.exp(normal.Normal(s[:,1] + a[:,1], p[1]).log_prob(s_[:,1]))\n",
    "            * (1 - torch.sigmoid(-beta * (p[0]-s[:,0])))\n",
    "        )\n",
    "    )\n",
    "        \n",
    "    loglike.backward()\n",
    "    \n",
    "    new_p = p.data + learning_rate * p.grad.data\n",
    "    # print(t, loglike.data.numpy(),new_p, p, p.grad.data)\n",
    "    if (np.abs(e) - np.abs(loglike.data.numpy())) <= e_margin:\n",
    "    # if torch.all(torch.lt((new_p - p.data), torch.Tensor([0.0001,0.0001,0.0001]))):\n",
    "        break\n",
    "    else:\n",
    "        if not new_p.isnan().any():\n",
    "            p.data = new_p\n",
    "            p.grad.data.zero_()\n",
    "            e = loglike.data.numpy()\n",
    "        else:\n",
    "            break\n",
    "\n",
    "\n",
    "print(\"loglike  =\", loglike.data.numpy(), \"theta =\", p.data.numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "init_p:  Parameter containing:\n",
      "tensor([7.4658, 7.9085, 7.8982], requires_grad=True)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Parameter containing:\n",
       "tensor([5.5656, 1.9258, 4.2426], requires_grad=True)"
      ]
     },
     "execution_count": 219,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def estimate(beta = 1.5, init_p=None):\n",
    "    s = Variable(torch.from_numpy(np.array(O)[:-1,0])).type(torch.DoubleTensor)\n",
    "    a = Variable(torch.from_numpy(np.array(O)[:-1,1])).type(torch.IntTensor)\n",
    "    s_ = Variable(torch.from_numpy(np.array(O)[1:,0])).type(torch.DoubleTensor)\n",
    "\n",
    "    if init_p is not None:\n",
    "        p = p = torch.nn.parameter.Parameter(torch.tensor(init_p))\n",
    "    else:\n",
    "        p = torch.nn.parameter.Parameter(torch.Tensor(3).uniform_(1, 10))\n",
    "    print('init_p: ', p)\n",
    "\n",
    "    e_margin = 1e-6\n",
    "    learning_rate = 0.002\n",
    "    e = - 1e+10\n",
    "    for t in range(10000):\n",
    "        # print(p)\n",
    "    \n",
    "        loglike = torch.sum(\n",
    "            torch.log(\n",
    "                torch.exp(normal.Normal(s[:,0] + a[:,0], p[2]).log_prob(s_[:,0]))\n",
    "                * torch.exp(normal.Normal(s[:,1] + a[:,1], p[2]).log_prob(s_[:,1]))\n",
    "                * torch.sigmoid(-beta * (p[0]-s[:,0]))\n",
    "                + \n",
    "                torch.exp(normal.Normal(s[:,0] + a[:,0], p[1]).log_prob(s_[:,0]))\n",
    "                * torch.exp(normal.Normal(s[:,1] + a[:,1], p[1]).log_prob(s_[:,1]))\n",
    "                * (1 - torch.sigmoid(-beta * (p[0]-s[:,0])))\n",
    "            )\n",
    "        )\n",
    "            \n",
    "        loglike.backward()\n",
    "        \n",
    "        new_p = p.data + learning_rate * p.grad.data\n",
    "        # print(t, loglike.data.numpy(),new_p, p, p.grad.data)\n",
    "        if (np.abs(e) - np.abs(loglike.data.numpy())) <= e_margin:\n",
    "        # if torch.all(torch.lt((new_p - p.data), torch.Tensor([0.0001,0.0001,0.0001]))):\n",
    "            break\n",
    "        else:\n",
    "            if not new_p.isnan().any():\n",
    "                p.data = new_p\n",
    "                p.grad.data.zero_()\n",
    "                e = loglike.data.numpy()\n",
    "            else:\n",
    "                break\n",
    "\n",
    "    # print(\"loglike  =\", loglike.data.numpy(), \"theta =\", p.data.numpy())\n",
    "\n",
    "    return p\n",
    "\n",
    "\n",
    "estimate(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 225,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Parameter containing:\n",
       "tensor([5.5514, 1.9257, 4.2273], requires_grad=True)"
      ]
     },
     "execution_count": 225,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def estimate(beta = 1.5, init_p=None):\n",
    "    s = Variable(torch.from_numpy(np.array(O)[:-1,0])).type(torch.DoubleTensor)\n",
    "    a = Variable(torch.from_numpy(np.array(O)[:-1,1])).type(torch.IntTensor)\n",
    "    s_ = Variable(torch.from_numpy(np.array(O)[1:,0])).type(torch.DoubleTensor)\n",
    "\n",
    "    if init_p is not None:\n",
    "        p = torch.nn.parameter.Parameter(torch.tensor(init_p))\n",
    "    else:\n",
    "        p = torch.nn.parameter.Parameter(torch.Tensor(3).uniform_(1, 10))\n",
    "        # p = torch.nn.parameter.Parameter(torch.randn(3))\n",
    "        # with torch.no_grad():\n",
    "        #         p[:] = p.clamp(min=1)\n",
    "        # while p[0] <= 0 or p[1] <= 0 or p[2] <= 0:\n",
    "        #     p = Variable(torch.randn(3), requires_grad=True)\n",
    "    # print('init_p: ', p)\n",
    "\n",
    "    loglike = lambda: -torch.sum(\n",
    "            torch.log(\n",
    "            torch.exp(normal.Normal(s[:,0] + a[:,0], p[2]).log_prob(s_[:,0]))\n",
    "            * torch.exp(normal.Normal(s[:,1] + a[:,1], p[2]).log_prob(s_[:,1]))\n",
    "            * torch.sigmoid(-beta * (p[0]-s[:,0]))\n",
    "            + \n",
    "            torch.exp(normal.Normal(s[:,0] + a[:,0], p[1]).log_prob(s_[:,0]))\n",
    "            * torch.exp(normal.Normal(s[:,1] + a[:,1], p[1]).log_prob(s_[:,1]))\n",
    "            * (1 - torch.sigmoid(-beta * (p[0]-s[:,0])))\n",
    "        )\n",
    "    )\n",
    "\n",
    "    # optim = torch.optim.SGD([p], lr=1e-2, momentum=0.9)\n",
    "    optim = torch.optim.SGD([p], lr=1e-2)\n",
    "\n",
    "    for i in range(1000):\n",
    "        # print(i, p)\n",
    "        ll = loglike()\n",
    "        optim.zero_grad()\n",
    "        ll.backward()\n",
    "        optim.step() \n",
    "\n",
    "        with torch.no_grad():\n",
    "            p[:] = p.clamp(min=1e-1)\n",
    "\n",
    "    return p\n",
    "\n",
    "\n",
    "estimate(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 233,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0 Parameter containing:\n",
      "tensor([1.0000, 2.4884, 2.4884], requires_grad=True)\n",
      "0.5 Parameter containing:\n",
      "tensor([8.2470, 1.9428, 4.9115], requires_grad=True)\n",
      "1.0 Parameter containing:\n",
      "tensor([6.5869, 1.9313, 4.5599], requires_grad=True)\n",
      "1.5 Parameter containing:\n",
      "tensor([5.8309, 1.9270, 4.3175], requires_grad=True)\n",
      "2.0 Parameter containing:\n",
      "tensor([5.5514, 1.9257, 4.2273], requires_grad=True)\n",
      "2.5 Parameter containing:\n",
      "tensor([5.4274, 1.9251, 4.1834], requires_grad=True)\n",
      "3.0 Parameter containing:\n",
      "tensor([5.3500, 1.9250, 4.1500], requires_grad=True)\n",
      "3.5 Parameter containing:\n",
      "tensor([5.2903, 1.9254, 4.1201], requires_grad=True)\n",
      "4.0 Parameter containing:\n",
      "tensor([5.2436, 1.9259, 4.0949], requires_grad=True)\n",
      "4.5 Parameter containing:\n",
      "tensor([5.2107, 1.9265, 4.0764], requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "start = [1., 1., 1.]\n",
    "\n",
    "for i in range(10):\n",
    "    print(i*.5, estimate(i*.5, start))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 253,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0 [2.37, 3.19, 1.62] [4.57, 1.62, 3.19] [5.14, 1.62, 3.19] [6.27, 1.62, 3.19] [5.37, 1.62, 3.19]\n",
      "0.5 [8.25, 1.94, 4.91] [8.28, 1.94, 4.92] [8.22, 1.94, 4.9] [8.33, 1.94, 4.94] [8.27, 1.94, 4.92]\n",
      "1.0 [6.59, 1.93, 4.56] [6.59, 1.93, 4.56] [6.59, 1.93, 4.56] [6.59, 1.93, 4.56] [6.59, 1.93, 4.56]\n",
      "1.5 [5.83, 1.93, 4.32] [5.83, 1.93, 4.32] [5.83, 1.93, 4.32] [5.83, 1.93, 4.32] [5.83, 1.93, 4.32]\n",
      "2.0 [5.55, 1.93, 4.23] [5.55, 1.93, 4.23] [5.55, 1.93, 4.23] [5.55, 1.93, 4.23] [5.55, 1.93, 4.23]\n",
      "2.5 [5.43, 1.93, 4.18] [5.43, 1.93, 4.18] [5.43, 1.93, 4.18] [5.43, 1.93, 4.18] [5.43, 1.93, 4.18]\n",
      "3.0 [5.35, 1.93, 4.15] [5.35, 1.93, 4.15] [5.35, 1.93, 4.15] [5.35, 1.93, 4.15] [5.35, 1.93, 4.15]\n",
      "3.5 [8.47, 2.19, 4.66] [5.29, 1.93, 4.12] [5.29, 1.93, 4.12] [5.29, 1.93, 4.12] [8.47, 2.19, 4.66]\n",
      "4.0 [5.24, 1.93, 4.09] [5.24, 1.93, 4.09] [5.24, 1.93, 4.09] [5.24, 1.93, 4.09] [5.24, 1.93, 4.09]\n",
      "4.5 [8.52, 2.19, 4.64] [8.52, 2.19, 4.64] [8.52, 2.19, 4.64] [5.21, 1.93, 4.08] [5.21, 1.93, 4.08]\n"
     ]
    }
   ],
   "source": [
    "for i in range(10):\n",
    "    n = [[round(n, 2) for n in estimate(i*.5).data.tolist()] for _ in range(5)]\n",
    "    print(i*.5, *n)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "19d1d53a962d236aa061289c2ac16dc8e6d9648c89fe79f459ae9a3493bc67b4"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
