{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "adolescent-thickness",
   "metadata": {
    "id": "adolescent-thickness"
   },
   "source": [
    "# Identifica√ß√£o\n",
    "\n",
    "**Nome:** Guilherme Albarrans Leite\n",
    "\n",
    "**NUSP:** 11890922"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "voluntary-contamination",
   "metadata": {
    "id": "voluntary-contamination"
   },
   "source": [
    "# MAC0434 e MAC5788 - Planejamento e Aprendizado por Refor√ßo (2023)\n",
    "\n",
    "\n",
    "## Aprendizado por refor√ßo profundo\n",
    "\n",
    "Nesse notebook iremos implementar o algoritmo de controle DQN, um algoritmo de aprendizado por refor√ßo profundo, que combina redes neurais e programa√ß√£o din√¢mica aproximada. A proposta geral do DQN √© combinar a regra de atualiza√ß√£o do Q-Learning com aproximadores de fun√ß√£o n√£o-lineares para se obter boa generaliza√ß√£o sobre o conjunto de poss√≠veis observa√ß√µes durante epis√≥dios de intera√ß√£o do agente com o ambiente.\n",
    "\n",
    "<img src=\"https://github.com/thiagopbueno/curso-verao-rl-ime-2021/blob/master/notebooks/aula3/img/control.png?raw=true\" alt=\"Agent-Env Loop\" style=\"width: 300px;\"/>\n",
    "\n",
    "Neste notebook o nosso objetivo ser√° resolver o ambiente `CartPole-v1`.\n",
    "\n",
    "### Objetivos:\n",
    "\n",
    "- Entender o papel da otimalidade de Bellman para algoritmos de controle\n",
    "- Desenvolver intui√ß√£o sobre o problema de explora√ß√£o/explota√ß√£o em RL\n",
    "- Ter um primeiro contato com t√©cnicas de treinamento de algoritmos de deep RL\n",
    "- Familiarizar-se com a biblioteca de redes neurais dm-sonnet"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "illegal-title",
   "metadata": {
    "id": "illegal-title"
   },
   "source": [
    "### üíª Configura√ß√µes do Ambiente"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "distinct-liberty",
   "metadata": {
    "id": "distinct-liberty"
   },
   "source": [
    "O c√≥digo utilizado na aula pr√°tica foi desenvolvido em 2021. √â importante observar que as vers√µes das bibliotecas utilizadas n√£o s√£o necessariamente as vers√µes mais recentes, e o uso de bibliotecas com vers√µes distintas das especificadas pode resultar em conflitos no c√≥digo.\n",
    "\n",
    "\n",
    "A vers√£o do Python utilizada √© a 3.7, e as vers√µes das bibliotecas est√£o especificadas no arquivo \"requirements.txt\".\n",
    "\n",
    "Para garantir que n√£o haja conflitos com o c√≥digo disponibilizado, sugerimos a instala√ß√£o dos pacotes e bibliotecas utilizados em algum ambiente virtual de sua escolha. Uma possibilidade √© criar um ambiente virtual utilizando o ```virtualenv``` conforme instru√ß√µes dispon√≠veis no Moodle."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd799a37",
   "metadata": {
    "id": "dd799a37"
   },
   "source": [
    "# üíª Execu√ß√£o do C√≥digo\n",
    "\n",
    "Ap√≥s configurar o ambiente, execute a c√©lula abaixo que importa as bibliotecas necess√°rias."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "pleased-vacuum",
   "metadata": {
    "id": "pleased-vacuum"
   },
   "outputs": [],
   "source": [
    "from collections import deque\n",
    "from datetime import datetime\n",
    "import os.path as osp\n",
    "import time\n",
    "\n",
    "import gym\n",
    "from gym.wrappers import AtariPreprocessing, FrameStack, Monitor, TimeLimit\n",
    "import numpy as np\n",
    "import sonnet as snt\n",
    "import tensorflow as tf\n",
    "from tqdm.notebook import trange\n",
    "\n",
    "from utils import logging\n",
    "from utils.nn import initializers\n",
    "from utils import replay\n",
    "from utils import schedule\n",
    "from utils import tf_utils\n",
    "\n",
    "#tf_utils.set_tf_allow_growth() # necess√°rio apenas se voc√™ disp√µe de GPU"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "worldwide-disco",
   "metadata": {
    "id": "worldwide-disco"
   },
   "source": [
    "## Ambiente - CartPole-v1\n",
    "\n",
    "Como veremos neste exerc√≠cio-programa, √© sempre uma boa ideia em aprendizado por refor√ßo iniciar o estudo de um algoritmo por um problema simples e pequeno para o qual voc√™ poder√° resolver em poucos minutos. Para isso, o ambiente do `CartPole-v1` √© usualmente um dos primeiros problemas que um agente baseado em aprendizado por refor√ßo deve ser capaz de resolver antes de tentar atacar problemas mais complexos.\n",
    "\n",
    "> Obs. Note que, por se tratar de uma implementa√ß√£o de 2021, a biblioteca utilizada para a importa√ß√£o do ambiente do CartPole √© a `gym` ao inv√©s da biblioteca [gymnasium](https://gymnasium.farama.org/index.html), que foi utilizada no EP2 (2023)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "bearing-assistant",
   "metadata": {
    "id": "bearing-assistant"
   },
   "outputs": [],
   "source": [
    "def make_envs(env_id, n=20): \n",
    "    env = gym.make(env_id) # ambiente que ser√° usado no treinamento\n",
    "    eval_env = gym.vector.make(env_id, num_envs=n, asynchronous=True) # ambiente de avalia√ß√£o\n",
    "    test_env = gym.make(env_id) # ambiente de teste\n",
    "    return env, eval_env, test_env"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "military-physiology",
   "metadata": {
    "id": "military-physiology"
   },
   "outputs": [],
   "source": [
    "env, eval_env, test_env = make_envs(\"CartPole-v1\", n=10)# originalmente era n=20, mas n√£o terminava a execuss√£o mesmo ap√≥s 7h rodando"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "parental-court",
   "metadata": {
    "id": "parental-court"
   },
   "source": [
    "## Deep Q-Learning\n",
    "\n",
    "Como visto em aula o algoritmo `DQN` procura aproximar a fun√ß√£o $Q(s, a)$ utilizando redes neurais treinadas por meio da otimiza√ß√£o de uma fun√ß√£o objetivo baseada na regra de atualiza√ß√£o do Q-Learning. O algoritmo abaixo descreve de maneira geral o treinamento de um agente `DQN`.\n",
    "\n",
    "<img src=\"https://github.com/thiagopbueno/curso-verao-rl-ime-2021/blob/master/notebooks/aula3/img/dqn-algo.png?raw=true\"/>\n",
    "\n",
    "Nessa se√ß√£o desenvolveremos os componentes deste algoritmo passo a passo:\n",
    "\n",
    "1. **Redes neurais (networks)**: inicialmente construiremos a rede neural para a fun√ß√£o $Q(s, a)$, usando a biblioteca `dm-sonnet`;\n",
    "2. **Fun√ß√£o objetivo (loss)**: uma vez definida a classe da fun√ß√£o $Q(s, a)$, implementaremos a fun√ß√£o objetivo utilizada no problema de \"regress√£o\" que o Q-Learning tenta resolver;\n",
    "3. **Atualiza√ß√£o (update)**: em seguida instanciaremos um otimizador baseado em gradientes que ser√° respons√°vel por minimizar a fun√ß√£o objetivo previamente definida; e\n",
    "4. **Pol√≠tica $\\epsilon$-greedy**: por fim, definiremos a pol√≠tica estoc√°stica para explora√ß√£o."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "saved-organic",
   "metadata": {
    "id": "saved-organic"
   },
   "source": [
    "### Redes Neurais (networks)\n",
    "\n",
    "Para representar fun√ß√µes $Q(s,a)$ utilizando redes neurais, temos em geral 2 op√ß√µes de implementa√ß√£o:\n",
    "1. Definir uma rede com entrada $(s, a)$ e sa√≠da um √∫nico n√∫mero real: $Q_\\phi : \\mathcal{S} \\times \\mathcal{A} \\rightarrow \\mathbb{R}$; ou\n",
    "2. Definir como entrada apenas o estado $s$ e sa√≠da um vetor de tamanho $|\\mathcal{A}|$: $Q_\\phi : \\mathcal{S} \\rightarrow \\mathbb{R}^{|\\mathcal{A}|}$ .\n",
    "\n",
    "Em geral, para algoritmos baseados no `DQN` √© costume utilizar a 2a op√ß√£o.\n",
    "\n",
    "> **Observa√ß√£o**: note que $\\phi \\in \\mathbb{R}^d$ com $d \\ll |S|$, onde $\\phi$ denota o conjunto de par√¢metros (i.e., *kernels* e *biases*) da rede neural. Dessa forma, a rede deve extrair apenas informa√ß√µes essenciais sobre o estado para a predi√ß√£o do retorno esperado (como vimos na aula de predi√ß√£o).\n",
    "\n",
    "<img src=\"https://github.com/thiagopbueno/curso-verao-rl-ime-2021/blob/master/notebooks/aula3/img/conv-net.png?raw=true\"/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "stylish-business",
   "metadata": {
    "id": "stylish-business"
   },
   "outputs": [],
   "source": [
    "class QNetwork(snt.Module):\n",
    "\n",
    "    def __init__(self, observation_space, action_space, name=\"QNetwork\"):\n",
    "        super().__init__(name=name)\n",
    "\n",
    "        self.observation_space = observation_space\n",
    "        self.action_space = action_space\n",
    "\n",
    "        # features\n",
    "        \"\"\"Defini√ß√£o das camadas ocultas da rede. 2 camadas com 8 neur√¥nios cada.\"\"\"\n",
    "        self._torso = snt.nets.MLP(\n",
    "            [8, 8],\n",
    "            activation=tf.nn.relu,\n",
    "            activate_final=True,\n",
    "            w_init=initializers.he_initializer(),\n",
    "            name=\"MLP\"\n",
    "        )\n",
    "\n",
    "        # predictor\n",
    "        \"\"\"Rede completamente conectada.\"\"\"\n",
    "        self._q_values = snt.Linear(action_space.n, name=\"QValues\")\n",
    "\n",
    "    @tf.function\n",
    "    def __call__(self, obs):\n",
    "        \"\"\"Calcula os Q-values de todas as a√ß√µes para uma dada `obs`.\"\"\"\n",
    "        h = self._torso(obs)\n",
    "        return self._q_values(h)\n",
    "\n",
    "    @tf.function\n",
    "    def action_values(self, obs, actions):\n",
    "        \"\"\"Calcula os Q-values de uma √∫nica `action` espec√≠fica para uma dada `obs`.\"\"\"\n",
    "        batch_size = tf.shape(obs)[0]\n",
    "        indices = tf.stack([tf.range(batch_size, dtype=actions.dtype), actions], axis=1)\n",
    "        q_values = tf.gather_nd(self(obs), indices)\n",
    "        return q_values\n",
    "\n",
    "    @tf.function\n",
    "    def hard_update(self, other):\n",
    "        \"\"\"Copia os par√¢metros da rede `other` para a rede do objeto.\"\"\"\n",
    "        for self_var, other_var in zip(self.trainable_variables, other.trainable_variables):\n",
    "            self_var.assign(other_var)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "metropolitan-flood",
   "metadata": {
    "id": "metropolitan-flood"
   },
   "source": [
    "### Fun√ß√£o objetivo (loss)\n",
    "\n",
    "Lembre-se que o `DQN` se utiliza da regra de atualiza√ß√£o baseado em programa√ß√£o din√¢mica aproximada do Q-Learning:\n",
    "\n",
    "$$\n",
    "\\mathcal{L}(\\phi) = \\mathbb{E}_{(s, a, r, s') \\sim \\mathcal{D}} [(Q_{\\phi}(s, a) - (r + \\gamma \\max_{a'} Q_{\\bar{\\phi}}(s', a')))^2]\n",
    "$$\n",
    "\n",
    "> **Observa√ß√£o**: lembre que para compor o \"alvo\" (target) da regress√£o usamos a rede target $Q_{\\bar{\\phi}}$. Conforme vimos na aula te√≥rica, o uso de *target networks* √© fundamental para melhorar a estabilidade do treinamento. Caso contr√°rio, toda atualiza√ß√£o na dire√ß√£o de $\\nabla_{\\phi} \\mathcal{L}(\\phi)$ acabaria por alterar tamb√©m o valor do \"alvo\" da regress√£o, tornando o problema de otimiza√ß√£o muito mais complicado!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "civil-durham",
   "metadata": {
    "id": "civil-durham"
   },
   "outputs": [],
   "source": [
    "def make_q_learning_loss(q_net, target_q_net, gamma=0.99):\n",
    "    \"\"\"Recebe a rede online `q_net` e a rede `target_q_net` e devolve o loss function do Q-Learning.\"\"\"\n",
    "\n",
    "    @tf.function\n",
    "    def _loss(batch):\n",
    "        \"\"\"Recebe um batch de experi√™ncias e devolve o valor da fun√ß√£o objetivo para esse batch.\"\"\"\n",
    "        obs = batch[\"obs\"]\n",
    "        actions = batch[\"action\"]\n",
    "        rewards = batch[\"reward\"]\n",
    "        next_obs = batch[\"next_obs\"]\n",
    "        terminals = tf.cast(batch[\"terminal\"], tf.float32)\n",
    "\n",
    "        # predictions\n",
    "        q_values = q_net.action_values(obs, actions)\n",
    "\n",
    "        # targets\n",
    "        next_q_values = tf.reduce_max(target_q_net(next_obs), axis=-1)\n",
    "        q_targets = tf.stop_gradient(rewards + (1 - terminals) * gamma * next_q_values)\n",
    "\n",
    "        # loss = tf.reduce_mean((q_values - q_targets) ** 2)\n",
    "        loss = tf.losses.huber(q_values, q_targets)\n",
    "        return loss\n",
    "\n",
    "    return _loss"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "composed-regression",
   "metadata": {
    "id": "composed-regression"
   },
   "source": [
    "### Atualiza√ß√£o (updates)\n",
    "\n",
    "Uma vez com *loss function* definida, basta instanciar um otimizador escolhendo uma taxa de aprendizado (i.e., `learning_rate`) rodando a c√©lula abaixo:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "blank-watson",
   "metadata": {
    "id": "blank-watson"
   },
   "outputs": [],
   "source": [
    "def make_update_fn(loss_fn, trainable_variables, learning_rate=1e-3):\n",
    "    optimizer = snt.optimizers.Adam(learning_rate)\n",
    "\n",
    "    @tf.function\n",
    "    def _update_fn(batch):\n",
    "        with tf.GradientTape(watch_accessed_variables=False) as tape:\n",
    "            tape.watch(trainable_variables)\n",
    "            loss = loss_fn(batch)\n",
    "\n",
    "        grads = tape.gradient(loss, trainable_variables)\n",
    "        optimizer.apply(grads, trainable_variables)\n",
    "\n",
    "        grads_and_vars = {var.name: (grad, var) for grad, var in zip(grads, trainable_variables)}\n",
    "\n",
    "        return loss, grads_and_vars\n",
    "\n",
    "    return _update_fn"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "remarkable-decrease",
   "metadata": {
    "id": "remarkable-decrease"
   },
   "source": [
    "### Pol√≠tica $\\epsilon$-*greedy*\n",
    "\n",
    "O √∫ltimo componente do algoritmo `DQN` √© sua pol√≠tica utilizada para explorar. No notebook de hoje, implementaremos a pol√≠tica explorat√≥ria mais simples.\n",
    "\n",
    "Como vimos na aula te√≥rica, a pol√≠tica $\\epsilon$-*greedy* escolhe uma a√ß√£o aleat√≥ria com probabilidade $\\epsilon$ e escolhe a a√ß√£o gulosa com probabilidade $1 - \\epsilon$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "endless-tyler",
   "metadata": {
    "id": "endless-tyler"
   },
   "outputs": [],
   "source": [
    "class EpsilonGreedyPolicy:\n",
    "\n",
    "    def __init__(self, q_net, start_val=1.0, end_val=0.01, start_step=1_000, end_step=10_000):\n",
    "        self.q_net = q_net\n",
    "\n",
    "        self._schedule = schedule.PiecewiseLinearSchedule((start_step, start_val), (end_step, end_val))\n",
    "\n",
    "        self._step = tf.Variable(0., dtype=tf.float32, name=\"step\")\n",
    "        self._epsilon = tf.Variable(start_val, dtype=tf.float32, name=\"epsilon\")\n",
    "\n",
    "    def __call__(self, obs):\n",
    "        \"\"\"Retorna a√ß√£o aleat√≥ria com probabilidade epsilon, c.c., retorna a√ß√£o gulosa.\"\"\"\n",
    "        self._epsilon.assign(self._schedule(self._step))\n",
    "        self._step.assign_add(1)\n",
    "\n",
    "        batch_size = tf.shape(obs)[0]\n",
    "        action_dim = self.q_net.action_space.n\n",
    "\n",
    "        random_actions = tf.random.uniform(shape=(batch_size,), minval=0, maxval=action_dim, dtype=tf.int32)\n",
    "        greedy_actions = tf.argmax(self.q_net(obs), axis=-1, output_type=tf.int32)\n",
    "\n",
    "        return tf.where(\n",
    "            self._epsilon > tf.random.uniform(shape=(batch_size,)),\n",
    "            random_actions,\n",
    "            greedy_actions\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "valuable-emerald",
   "metadata": {
    "id": "valuable-emerald"
   },
   "source": [
    "### Agente DQN\n",
    "\n",
    "Com todos os componentes definidos, estamos finalmente preparados para instanciar um agente `DQN` para o ambiente `CartPole-v1`. Execute a c√©lula abaixo para criar a classe `DQN`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "cleared-accident",
   "metadata": {
    "id": "cleared-accident"
   },
   "outputs": [],
   "source": [
    "class DQN:\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        observation_space,\n",
    "        action_space,\n",
    "        gamma=0.99,\n",
    "        target_update_freq=1000,\n",
    "        learning_rate=1e-3,\n",
    "        policy_start_val=1.0,\n",
    "        policy_end_val=0.01,\n",
    "        policy_start_step=1_000,\n",
    "        policy_end_step=10_000,\n",
    "        checkpoint_dir=\"ckpt\"\n",
    "    ):\n",
    "        self.observation_space = observation_space\n",
    "        self.action_space = action_space\n",
    "\n",
    "        self.gamma = gamma\n",
    "        self.target_update_freq = target_update_freq\n",
    "        self.learning_rate = learning_rate\n",
    "\n",
    "        self.q_net = QNetwork(self.observation_space, self.action_space, name=\"QNet\")\n",
    "        self.target_q_net = QNetwork(self.observation_space, self.action_space, name=\"TargetQNet\")\n",
    "\n",
    "        self.policy = EpsilonGreedyPolicy(self.q_net, start_val=policy_start_val, end_val=policy_end_val, start_step=policy_start_step, end_step=policy_end_step)\n",
    "\n",
    "        self._ckpt_dir = checkpoint_dir\n",
    "        self._ckpt = tf.train.Checkpoint(q_net=self.q_net)\n",
    "        self._ckpt_manager = tf.train.CheckpointManager(self._ckpt, directory=self._ckpt_dir, max_to_keep=1)\n",
    "\n",
    "        self._step = tf.Variable(0, dtype=tf.int32, name=\"step\")\n",
    "\n",
    "    def build(self):\n",
    "        \"\"\"Cria as vari√°veis das redes online e target e sincroniza inicialmente.\"\"\"\n",
    "        input_spec = tf.TensorSpec(self.observation_space.shape, dtype=tf.float32)\n",
    "        tf_utils.create_variables(self.q_net, input_spec)\n",
    "        tf_utils.create_variables(self.target_q_net, input_spec)\n",
    "        self.target_q_net.hard_update(self.q_net)\n",
    "\n",
    "    def compile(self):\n",
    "        \"\"\"Compila a DQN loss junto com a Q-network.\"\"\"\n",
    "        self.update_learner = make_update_fn(\n",
    "            make_q_learning_loss(self.q_net, self.target_q_net, gamma=self.gamma),\n",
    "            self.q_net.trainable_variables,\n",
    "            learning_rate=self.learning_rate\n",
    "        )\n",
    "\n",
    "    def step(self, obs, training=True):\n",
    "        \"\"\"Escolhe a a√ß√£o para a observa√ß√£o dada.\"\"\"\n",
    "        obs = tf.convert_to_tensor(obs, dtype=tf.float32)\n",
    "        action = self.policy(obs) if training else tf.argmax(self.q_net(obs), axis=-1)\n",
    "        return action.numpy()\n",
    "\n",
    "    def learn(self, batch):\n",
    "        \"\"\"Recebe um batch de experi√™ncias, atualiza os par√¢metros das redes, e devolve algumas m√©tricas.\"\"\"\n",
    "        # atualiza q_net\n",
    "        loss, grads_and_vars = self.update_learner(batch)\n",
    "\n",
    "        # sincroniza target_q_net\n",
    "        self._step.assign_add(1)\n",
    "        if self._step % self.target_update_freq == 0:\n",
    "            self.target_q_net.hard_update(self.q_net)\n",
    "\n",
    "        # m√©tricas de monitoramento\n",
    "        stats = {\n",
    "            \"loss\": loss,\n",
    "            \"q_values_mean\": tf.reduce_mean(self.q_net(batch[\"obs\"])),\n",
    "            \"epsilon\": self.policy._epsilon,\n",
    "            \"vars\": {key: variable for key, (_, variable) in grads_and_vars.items()},\n",
    "            \"grads\": {f\"grad_{key}\": grad for key, (grad, _) in grads_and_vars.items()},\n",
    "        }\n",
    "\n",
    "        return stats\n",
    "\n",
    "    def save(self):\n",
    "        \"\"\"Salva o estado atual do agente (i.e., o valor dos par√¢metros da rede online) nesse momento.\"\"\"\n",
    "        return self._ckpt_manager.save()\n",
    "\n",
    "    def restore(self, save_path=None):\n",
    "        \"\"\"Carrega o √∫ltimo checkpoint salvo anteriormente no `save_path`.\"\"\"\n",
    "        if not save_path:\n",
    "            save_path = self._ckpt_manager.latest_checkpoint\n",
    "        return self._ckpt.restore(save_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "authorized-navigation",
   "metadata": {
    "id": "authorized-navigation"
   },
   "source": [
    "## Protocolo de treinamento, avalia√ß√£o e teste\n",
    "\n",
    "Com a classe do `DQN` definida √© hora de treinar o agente e avali√°-lo. Faremos isso seguindo um protocolo de treinamento e avalia√ß√£o definido pelas fun√ß√µes `train` e  `evaluate` abaixo.\n",
    "\n",
    "<img src=\"https://github.com/thiagopbueno/curso-verao-rl-ime-2021/blob/master/notebooks/aula3/img/rl-training.png?raw=true\"/>\n",
    "\n",
    "Tente entender como os hiperpar√¢metros de in√≠cio de treinamento `learning_starts` e frequ√™ncia de atualiza√ß√µes `learn_every` e avalia√ß√£o `evaluation_freq` definem o protocolo.\n",
    "\n",
    "> **Observa√ß√£o**: note que embora o protocolo abaixo seja bastante gen√©rico, tenha em mente que diferentes trabalhos alteram a maneira como os processos de coleta de dados, aprendizado e avalia√ß√£o se intercalam."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "mobile-sixth",
   "metadata": {
    "id": "mobile-sixth"
   },
   "outputs": [],
   "source": [
    "def train(\n",
    "    agent,\n",
    "    env,\n",
    "    test_env,\n",
    "    replay,\n",
    "    logger,\n",
    "    total_timesteps=20_000,\n",
    "    learning_starts=1_000,\n",
    "    learn_every=1,\n",
    "    evaluation_freq=1_000\n",
    "):\n",
    "    timesteps = 0\n",
    "    episodes = 0\n",
    "    episode_returns = deque(maxlen=100)\n",
    "\n",
    "    best_episode_reward_mean = -np.inf\n",
    "\n",
    "    with trange(total_timesteps, desc=\"training\") as pbar:\n",
    "\n",
    "        while timesteps < total_timesteps:\n",
    "            obs = env.reset()\n",
    "            episode_return = 0.0\n",
    "\n",
    "            for episode_length in range(1, env.spec.max_episode_steps + 1):\n",
    "\n",
    "                # collect\n",
    "                action = agent.step(np.expand_dims(obs, axis=0), training=True)[0]\n",
    "                next_obs, reward, done, info = env.step(action)\n",
    "\n",
    "                timesteps += 1\n",
    "                episode_return += reward\n",
    "\n",
    "                # add experience to replay buffer\n",
    "                terminal = done if episode_length < env.spec.max_episode_steps else False\n",
    "                replay.add(obs, action, reward, terminal, next_obs)\n",
    "\n",
    "                # training\n",
    "                if timesteps >= learning_starts and timesteps % learn_every == 0:\n",
    "                    batch = replay.sample()\n",
    "                    stats = agent.learn(batch)\n",
    "                    stats[\"episode_return_mean\"] = np.mean(episode_returns)\n",
    "                    logger.log(timesteps, stats, label=\"train\") # logging\n",
    "\n",
    "                # evaluation\n",
    "                if timesteps % evaluation_freq == 0:\n",
    "                    stats = evaluate(agent, test_env)\n",
    "                    logger.log(timesteps, stats, label=\"evaluation\") # logging\n",
    "\n",
    "                    # checkpointing\n",
    "                    if stats[\"episode_return_mean\"] > best_episode_reward_mean:\n",
    "                        agent.save()\n",
    "                        best_episode_reward_mean = stats[\"episode_return_mean\"]\n",
    "\n",
    "                if done:\n",
    "                    break\n",
    "\n",
    "                obs = next_obs\n",
    "\n",
    "            episodes += 1\n",
    "            episode_returns.append(episode_return)\n",
    "\n",
    "            # logging\n",
    "            stats = {\n",
    "                \"episodes\": episodes,\n",
    "                \"episode_length\": episode_length,\n",
    "                \"episode_return\": episode_return,\n",
    "            }\n",
    "            logger.log(timesteps, stats, label=\"collect\")\n",
    "            logger.flush()\n",
    "\n",
    "            pbar.update(episode_length)\n",
    "            pbar.set_postfix(timesteps=timesteps, episodes=episodes, avg_returns=np.mean(episode_returns) if episode_returns else None)\n",
    "\n",
    "    # final evaluation\n",
    "    stats = evaluate(agent, test_env)\n",
    "    logger.log(timesteps, stats, label=\"evaluation\")\n",
    "    logger.flush()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "similar-tablet",
   "metadata": {
    "id": "similar-tablet"
   },
   "source": [
    "Para avaliarmos o agente, utilizaremos o `eval_env` que foi criado como um ambiente paralelizado (contendo `env.num_envs` rodando de forma ass√≠ncrona em paralelo).\n",
    "\n",
    "> **Observa√ß√£o**: Note no c√≥digo abaixo, como esse tipo de ambiente altera ligeiramente o ciclo de intera√ß√£o agente-ambiente que vimos nas √∫ltimas aulas. Para maiores detalhes, consulte a documenta√ß√£o de `gym.vector.make` e o c√≥digo dos m√≥dulos em [https://github.com/openai/gym/tree/master/gym/vector](https://github.com/openai/gym/tree/master/gym/vector)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "combined-delight",
   "metadata": {
    "id": "combined-delight"
   },
   "outputs": [],
   "source": [
    "def evaluate(agent, env):\n",
    "    total_reward = np.zeros((env.num_envs,))\n",
    "    episode_length = np.zeros((env.num_envs,))\n",
    "\n",
    "    obs = env.reset()\n",
    "    dones = np.array([False] * env.num_envs)\n",
    "\n",
    "    while not np.all(dones):\n",
    "        action = agent.step(obs, training=False)\n",
    "        obs, reward, done, _ = env.step(action)\n",
    "        total_reward += (1 - dones) * reward\n",
    "        episode_length += (1 - dones)\n",
    "        dones = np.logical_or(dones, done)\n",
    "\n",
    "    return {\n",
    "        \"episode_return_mean\": np.mean(total_reward),\n",
    "        \"episode_return_min\": np.min(total_reward),\n",
    "        \"episode_return_max\": np.max(total_reward),\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "substantial-plain",
   "metadata": {
    "id": "substantial-plain"
   },
   "source": [
    "Execute a c√©lula abaixo para definir um ciclo de intera√ß√£o agente-ambiente para renderizar epis√≥dios do agente ap√≥s o treinamento e ent√£o verificar qualitativamente qu√£o boa foi a pol√≠tica que o agente aprendeu."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "descending-connectivity",
   "metadata": {
    "id": "descending-connectivity"
   },
   "outputs": [],
   "source": [
    "from collections import namedtuple\n",
    "\n",
    "TestEpisode = namedtuple(\"TestEpisode\", [\"number\", \"length\", \"accumulated_reward\"])\n",
    "\n",
    "def test(agent, env, episodes=3, wait=None, render=True, print_results=False):\n",
    "    results = []\n",
    "\n",
    "    for episode in range(episodes):\n",
    "        done = False\n",
    "        obs = env.reset()\n",
    "\n",
    "        if render:\n",
    "            env.render()\n",
    "\n",
    "        episode_len = 0\n",
    "        accumulated_reward = 0\n",
    "\n",
    "        while not done:\n",
    "            action = agent.step(np.expand_dims(obs, axis=0), training=False)[0]\n",
    "            obs, reward, done, _ = env.step(action)\n",
    "\n",
    "            episode_len += 1\n",
    "            accumulated_reward += reward\n",
    "\n",
    "            if render:\n",
    "                env.render()\n",
    "\n",
    "            if wait:\n",
    "                time.sleep(wait)\n",
    "\n",
    "        episode_result = TestEpisode(episode+1, episode_len, accumulated_reward)\n",
    "        results.append(episode_result)\n",
    "\n",
    "        if print_results:\n",
    "          print(f\"Episode {episode_result.number}\")\n",
    "          print(f\"Length: {episode_result.length}\")\n",
    "          print(f\"Accumulated reward: {episode_result.accumulated_reward}\")\n",
    "          print()\n",
    "\n",
    "    env.close()\n",
    "\n",
    "    return results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "informative-reservoir",
   "metadata": {
    "id": "informative-reservoir"
   },
   "source": [
    "## Treinando DQN no CartPole-v1\n",
    "\n",
    "Finalmente, temos todo o c√≥digo necess√°rio para treinarmos o `DQN` no `CartPole-v1`. Antes de iniciarmos o treinamento, execute a c√©lula abaixo para instanciarmos o `tensorboard`, a ferramenta de *logging* e monitoramento do TensorFlow. Consulte a documenta√ß√£o e os tutoriais dispon√≠veis em [https://www.tensorflow.org/tensorboard](https://www.tensorflow.org/tensorboard) para maiores informa√ß√µes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "casual-alpha",
   "metadata": {
    "id": "casual-alpha"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The tensorboard extension is already loaded. To reload it, use:\n",
      "  %reload_ext tensorboard\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Reusing TensorBoard on port 6006 (pid 21264), started 9 days, 9:01:46 ago. (Use '!kill 21264' to kill it.)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "      <iframe id=\"tensorboard-frame-ac6ace855934c13e\" width=\"100%\" height=\"800\" frameborder=\"0\">\n",
       "      </iframe>\n",
       "      <script>\n",
       "        (function() {\n",
       "          const frame = document.getElementById(\"tensorboard-frame-ac6ace855934c13e\");\n",
       "          const url = new URL(\"http://localhost\");\n",
       "          const port = 6006;\n",
       "          if (port) {\n",
       "            url.port = port;\n",
       "          }\n",
       "          frame.src = url;\n",
       "        })();\n",
       "      </script>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%load_ext tensorboard\n",
    "%tensorboard --logdir logs --reload_interval 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "competent-arlington",
   "metadata": {
    "id": "competent-arlington"
   },
   "outputs": [],
   "source": [
    "def run(env, eval_env, run_name=\"default\", trials=3,\n",
    "        total_timesteps=20_000, learning_starts=1_000, evaluation_freq=1_000,\n",
    "        gamma=0.99, target_update_freq=1000, learning_rate=1e-3,\n",
    "        policy_start_val=1.0, policy_end_val=0.01, policy_start_step=1_000, policy_end_step=10_000,\n",
    "        replay_buffer_batch_size=64):\n",
    "    for _ in range(trials):\n",
    "        timestamp = datetime.now().strftime(\"%Y-%m-%d-%H.%M\")\n",
    "        run_id = osp.join(f\"dqn-{env.spec.id}-{run_name}\".lower(), timestamp)\n",
    "\n",
    "        logger = logging.TFLogger(run_id, base_dir=\"logs\")\n",
    "\n",
    "        buffer = replay.ReplayBuffer(env.observation_space, env.action_space, max_size=total_timesteps, batch_size=replay_buffer_batch_size)\n",
    "        buffer.build()\n",
    "\n",
    "        agent = DQN(env.observation_space, env.action_space, checkpoint_dir=f\"ckpt/{run_id}\",\n",
    "                    gamma=gamma, target_update_freq=target_update_freq, learning_rate=learning_rate,\n",
    "                    policy_start_val=policy_start_val, policy_end_val=policy_end_val, policy_start_step=policy_start_step, policy_end_step=policy_end_step)\n",
    "\n",
    "        agent.build()\n",
    "        agent.compile()\n",
    "\n",
    "        train(agent, env, eval_env, buffer, logger, total_timesteps=total_timesteps, learning_starts=learning_starts, evaluation_freq=evaluation_freq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "finnish-organization",
   "metadata": {
    "id": "finnish-organization"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "79dbed7fa56241fb83f2eebce4eaf249",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "training:   0%|          | 0/20000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "run(env, eval_env, run_name=\"default\", trials=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "caroline-tours",
   "metadata": {
    "id": "caroline-tours"
   },
   "source": [
    "Agora √© s√≥ buscar um caf√© esperar o resultado do treinamento. ;)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "meaning-multiple",
   "metadata": {
    "id": "meaning-multiple"
   },
   "source": [
    "### Teste do Agente no CartPole-v1\n",
    "\n",
    "Escolha um dos agentes treinados acima para testar e visualizar seu comportamento com o c√≥digo abaixo:\n",
    "> Lembre-se de que, ap√≥s a conclus√£o do treinamento, ser√° gerada uma pasta denominada \"ckpt\" em seu computador, na qual ser√£o armazenadas as pol√≠ticas que foram aprendidas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "ideal-circuit",
   "metadata": {
    "id": "ideal-circuit"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[TestEpisode(number=1, length=9, accumulated_reward=9.0),\n",
       " TestEpisode(number=2, length=8, accumulated_reward=8.0),\n",
       " TestEpisode(number=3, length=10, accumulated_reward=10.0),\n",
       " TestEpisode(number=4, length=9, accumulated_reward=9.0),\n",
       " TestEpisode(number=5, length=9, accumulated_reward=9.0),\n",
       " TestEpisode(number=6, length=10, accumulated_reward=10.0),\n",
       " TestEpisode(number=7, length=24, accumulated_reward=24.0),\n",
       " TestEpisode(number=8, length=22, accumulated_reward=22.0),\n",
       " TestEpisode(number=9, length=10, accumulated_reward=10.0),\n",
       " TestEpisode(number=10, length=27, accumulated_reward=27.0)]"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "checkpoint_dir = \"ckpt/dqn-cartpole-v1-default/2023-11-28-20:02\" # altere essa linha para escolher qual checkpoint do agente\n",
    "\n",
    "agent = DQN(env.observation_space, env.action_space, checkpoint_dir=checkpoint_dir)\n",
    "agent.build()\n",
    "agent.restore()\n",
    "\n",
    "test(agent, test_env, episodes=10, render=False) #render True s√≥ funciona localmente"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "X14vamQ4bECR",
   "metadata": {
    "id": "X14vamQ4bECR"
   },
   "source": [
    "### üí° Tarefa: Treinando o DQN com outros hiper par√¢metros\n",
    "\n",
    "Esta se√ß√£o destina-se ao treinamento do agente, seguindo as orienta√ß√µes fornecidas nos slides do EP3 dispon√≠veis no Moodle. A seguir, inclua as c√©lulas com as especifica√ß√µes das execu√ß√µes que ser√£o testadas.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "xplxVsR2a1K5",
   "metadata": {
    "id": "xplxVsR2a1K5"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6f1470ebbf074c189a309e02e73d5744",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "training:   0%|          | 0/20000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "aac242b0d4c04f24b322a9ab2bd67674",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "training:   0%|          | 0/20000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "30d3f3d3545548bfbd4b2956fc67572e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "training:   0%|          | 0/20000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Exemplos de alguns hiper par√¢metros que podem ser alterados:\n",
    "trials = 3\n",
    "\n",
    "params = {\n",
    "    'total_timesteps': 20_000,\n",
    "    'policy_end_step': 10_000,\n",
    "    'replay_buffer_batch_size': 64\n",
    "}\n",
    "\n",
    "run_name=f\"default_trials\" # Altere este nome para melhor organiza√ß√£o dos resultados\n",
    "\n",
    "# Exemplo de execu√ß√£o\n",
    "run(env, eval_env, run_name, trials, **params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "daf00fa0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8c29f94b560b408683e834bde9ccf8c3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "training:   0%|          | 0/30000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ad00d1960d15400690c1cd23b62ddda7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "training:   0%|          | 0/30000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9d8e0ce5909d4b51b0a119ff4f6287a7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "training:   0%|          | 0/30000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Exemplos de alguns hiper par√¢metros que podem ser alterados:\n",
    "trials = 3\n",
    "\n",
    "params = {\n",
    "    'total_timesteps': 30_000,\n",
    "    'policy_end_step': 10_000,\n",
    "    'replay_buffer_batch_size': 64\n",
    "}\n",
    "\n",
    "run_name=f\"total_timesteps={params['total_timesteps']}\" # Altere este nome para melhor organiza√ß√£o dos resultados\n",
    "\n",
    "# Exemplo de execu√ß√£o\n",
    "run(env, eval_env, run_name, trials, **params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "e08cc9d5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a1fc7ec39c1441bfbbe4c9ef3efd9244",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "training:   0%|          | 0/20000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a01fa1b57fdd4cab9494c494d5f44f27",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "training:   0%|          | 0/20000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d027f5ead65c4226b52565412f5e8128",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "training:   0%|          | 0/20000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Exemplos de alguns hiper par√¢metros que podem ser alterados:\n",
    "trials = 3\n",
    "\n",
    "params = {\n",
    "    'total_timesteps': 20_000,\n",
    "    'policy_end_step': 5_000,\n",
    "    'replay_buffer_batch_size': 64\n",
    "}\n",
    "\n",
    "run_name=f\"policy_end_step={params['policy_end_step']}\" # Altere este nome para melhor organiza√ß√£o dos resultados\n",
    "\n",
    "# Exemplo de execu√ß√£o\n",
    "run(env, eval_env, run_name, trials, **params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "b9231043",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e161254281ed4ccd8d576fa87dddbec1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "training:   0%|          | 0/20000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2f729bd942ff4b79a00fe274ef31bd28",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "training:   0%|          | 0/20000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b6372a0899e0490b9924001fc1552250",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "training:   0%|          | 0/20000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Exemplos de alguns hiper par√¢metros que podem ser alterados:\n",
    "trials = 3\n",
    "\n",
    "params = {\n",
    "    'total_timesteps': 20_000,\n",
    "    'policy_end_step': 10_000,\n",
    "    'replay_buffer_batch_size': 96\n",
    "}\n",
    "\n",
    "run_name=f\"replay_buffer_batch_size={params['replay_buffer_batch_size']}\" # Altere este nome para melhor organiza√ß√£o dos resultados\n",
    "\n",
    "# Exemplo de execu√ß√£o\n",
    "run(env, eval_env, run_name, trials, **params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "eefe7af1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c12bab0cb0084ea09b7ffc66a40dda72",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "training:   0%|          | 0/30000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a87b40f8883c4a36be2623f38e254159",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "training:   0%|          | 0/30000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "57d33f1c78524124af659fe498e29991",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "training:   0%|          | 0/30000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Exemplos de alguns hiper par√¢metros que podem ser alterados:\n",
    "trials = 3\n",
    "\n",
    "params = {\n",
    "    'total_timesteps': 30_000,\n",
    "    'policy_end_step': 5_000,\n",
    "    'replay_buffer_batch_size': 96\n",
    "}\n",
    "\n",
    "run_name=f\"all\" # Altere este nome para melhor organiza√ß√£o dos resultados\n",
    "\n",
    "# Exemplo de execu√ß√£o\n",
    "run(env, eval_env, run_name, trials, **params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "93a09d78",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c6bd7aa355be456ca30aee56a5cbf7ab",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "training:   0%|          | 0/10000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "771e228bd842476980269efc81ad4d60",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "training:   0%|          | 0/10000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4801a9b41d544c048896a7b5c91d5b59",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "training:   0%|          | 0/10000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Exemplos de alguns hiper par√¢metros que podem ser alterados:\n",
    "\n",
    "trials = 3\n",
    "\n",
    "params = {\n",
    "    \"total_timesteps\": 10_000,\n",
    "    \"learning_starts\": 500,\n",
    "    \"evaluation_freq\": 100,\n",
    "    \"target_update_freq\": 100,\n",
    "    \"gamma\": 0.99,\n",
    "    \"learning_rate\": 1e-3,\n",
    "    \"policy_start_val\": 1.0,\n",
    "    \"policy_end_val\": 0.001,\n",
    "    \"policy_start_step\": 1000,\n",
    "    \"policy_end_step\": 6000,\n",
    "    \"replay_buffer_batch_size\": 128,\n",
    "}\n",
    "run_name = f\"test\" # Altere este nome para melhor organiza√ß√£o dos resultados\n",
    "\n",
    "# Exemplo de execu√ß√£o\n",
    "run(env, eval_env, run_name, trials, **params)"
   ]
  }
 ],
 "metadata": {
  "accelerator": "TPU",
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
